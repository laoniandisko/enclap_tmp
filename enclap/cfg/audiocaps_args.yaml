# Experiment Config for each experiment
output_dir: /data/jyk/aac_results/bart_large/audiocaps_3e5_gpu4_1115_2000
logging_dir: runs/tb_log
logging_steps: 10
seed: 1115
train_file: /workspace/audiobart/csv/AudioCaps/train.csv
validation_file: /workspace/audiobart/csv/AudioCaps/val.csv
test_file: /workspace/audiobart/csv/AudioCaps/test.csv
base_path: /data/jyk/aac_dataset/AudioCaps/encodec_16
clap_base_path: /data/jyk/aac_dataset/AudioCaps/clap_audio_fused
tokenizer_name: facebook/bart-large
# model_name_or_path: /workspace/audiobart/bart/model
model_name_or_path: facebook/bart-large
num_captions: 5
overwrite_output_dir: False


# Training Configs
# Basic Config
max_encodec_length: 1022
only_encoder_epochs: 0
only_encodec_epochs: 0
clap_masking_prob: -1
encodec_masking_prob: 0.15
encodec_masking_length: 10
random_sampling: true
num_train_epochs: 30
max_train_steps: null
gradient_accumulation_steps: 1
per_device_train_batch_size: 64
per_device_eval_batch_size: 64
split_batches: true
checkpointing_steps: epoch  # 'epoch' to save for each epoch, or number of steps
resume_from_checkpoint: null

# Model & Generation Config
max_source_length: 1024
max_target_length: 128
val_max_target_length: 50
num_beams: null
pad_to_max_length: false
num_subsampling: 0

# Training Hyperparameters
learning_rate: 3e-5   # peak lr
# Should be one of "linear", "cosine", "cosine_with_restarts", "polynomial", 
# "constant", "constant_with_warmpup", "inverse_sqrt", "reduce_lr_on_plateau", "two_stage_inverse_sqrt"
lr_scheduler_type: inverse_sqrt
# lr_scheduler_type: two_stage_inverse_sqrt
weight_decay: 0.01
num_warmup_steps: 2000
max_grad_norm: 1.0

# Do not Change 
with_tracking: true
report_to: all
ignore_pad_token_for_loss: true 
preprocessing_num_workers: 32
use_slow_tokenizer: false
overwrite_cache: false